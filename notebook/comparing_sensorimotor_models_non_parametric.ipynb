{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between learning sensorimotor models \n",
    "## Part 1 : non parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we compare the different sensorimotor models used in explauto in order to help the reader to use the most appropriate model depending on his aim. The notebook is split in two parts : non parametrics models (this notebook) and models based on gaussian mixtures (coming soon). There is also a tutorial explaining how to use the [SensorimotorModel abstract class](http://flowersteam.github.io/explauto/explauto.sensorimotormodel.html#explauto.sensorimotor_model.sensorimotor_model.SensorimotorModel) ([learning_sensorimotor_models](http://nbviewer.ipython.org/github/flowersteam/explauto/blob/master/notebook/learning_sensorimotor_models.ipynb)).\n",
    "\n",
    "As explained in the [Explauto introduction](http://flowersteam.github.io/explauto/about.html), an important challenge in Developmental Robotics is how robots can efficiently learn sensorimotor mappings by experience, i.e. the mappings between the motor actions they make and the sensory effects they produce. This can be a robot learning how arm movements make physical objects move, or how movements of a virtual vocal tract modulates vocalization sounds.\n",
    "\n",
    "Let's begin with defining a simple environment that will be used to test the sensorimotor models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from explauto import Environment\n",
    "environment = Environment.from_configuration('simple_arm', 'low_dimensional')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensorimotor models introduction\n",
    "\n",
    "In Explauto, a sensorimotor model implements both the iterative learning process from sensorimotor experience, i.e. from the iterative collection of $(m, s)$ pairs by interaction with the environment, and the use of the resulting internal model to perform forward and inverse predictions (or any kind of general prediction between sensorimotor subspaces). \n",
    "\n",
    "Learning sensorimotor mappings involves machine learning algorithms, for which Explauto provides a unified interface through the [SensorimotorModel abstract class](http://flowersteam.github.io/explauto/explauto.sensorimotormodel.html#explauto.sensorimotor_model.sensorimotor_model.SensorimotorModel). \n",
    "\n",
    "Using the simple arm environment above, it allows to iteratively learn a sensorimotor model which will be able to:\n",
    "* infer the position of the end-effector from a given motor command, what is called *forward prediction*,\n",
    "* infer the motor command allowing to reach a particular end-effector position, what is called *inverse prediction*.\n",
    "* update online from sensorimotor experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Environment definition\n",
    "from explauto.environment.environment import Environment\n",
    "environment = Environment.from_configuration('simple_arm', 'mid_dimensional')\n",
    "\n",
    "from explauto import SensorimotorModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "In order to use the most appropriate model, there are several questions that have to be asked before using the following decision tree. The following paragraphs focus on these questions.\n",
    "\n",
    "*Add the decision tree*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non parametric or gaussian mixture models ? \n",
    "\n",
    "Each of non parametric model is currently based on the nearest neigbhor look up. They are non paramtric models because they don't rely on assomptions that the data are drown from a given probability while gassian mixture models (GMM) assumes that the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. Readers in this case should refer to the follwing tutorial : *coming soon* .\n",
    "\n",
    "Available sensorimotor models in Explauto can be accessed using: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sensorimotor models: ['LWLR-BFGS', 'nearest_neighbor', 'WNN', 'LWLR-CMAES']\n"
     ]
    }
   ],
   "source": [
    "from explauto.sensorimotor_model import sensorimotor_models\n",
    "print 'Available sensorimotor models: {}'.format(sensorimotor_models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentation of the different non parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 4 non parametric models, which are a combination of a forward and an inverse model, are:\n",
    "* the **nearest neighbor** model searches the nearest point of a given input ($m$ or $s$) in the dataset and returns its corresponding value (respectively $s$ or $m$)\n",
    "* the **WNN or weighted nearest neighbor** model searches the $k$ nearest points of a given input ($m$ or $s$) in the dataset and returns the average of the $k$ corresponding values (respectively $m$ or $s$)\n",
    "* Both **LWLR-BFGS and LWLR-CMAES** use **the Locally Weigthed Linear Regression (LWLR)** for their forward models. It computes a linear regression of the $k$ nearest neighbors of $m$ and find the requested $s$ with the given $m$ based on that regression. Both inverse models are optimisation algorithm that minimize the error  $e(m) = ||LWLR(m) - s_g||^2$  where $s_g$ is the goal, $LWLR$ is the forward model LWLR, and $m$ is the motor command to be infered. BFGS and CMAES are inverse models which are going to be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eploit or explore mode ?\n",
    "\n",
    "All the non-parametric sensorimotor models have two operating modes: \"explore\" and \"exploit\".\n",
    "\n",
    "In the \"explore\" mode, when the agent asks for the exact inverse prediction $m$ of a goal $s_g$, $m$ will be perturbated with some gaussian exploration noise in order to allow the agent to explore new motor commands. The sensorimotor models thus have a common parameter: sigma_explo_ratio=0.1 (default), which is the standard deviation of the gaussian noise, scaled depending of the motor domain size: if a motor value is bounded in [-2:2], then a sigma_explo_ratio of 0.1 will induce an exploration noise of (m_max - m_min) * sigma_explo_ratio = 0.4\n",
    "\n",
    "In the \"exploit\" mode, no exploration noise is added. This mode is used for instance when evaluating the inverse model for comparison purposes.\n",
    "\n",
    "Each model does not have several modes. In order to know what are their different modes and caracteristics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': {'fwd': 'NN', 'inv': 'NN', 'sigma_explo_ratio': 0.1, 'mode': 'explore'}, 'exact': {'fwd': 'NN', 'inv': 'NN', 'sigma_explo_ratio': 0.0, 'mode': 'exploit'}}\n"
     ]
    }
   ],
   "source": [
    "sm_cls, sm_configs = sensorimotor_models['nearest_neighbor'] # or 'WNN', 'LWLR-BFGS' or 'LWLR-CAMES'\n",
    "print sm_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it is possible to change its caracteristics like sigma exploration ratio for nearest neighbor models or the number $k$ of nearest neighbor in WNN models this way : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm_configs['my_mode']['my_caracteristic'] = my_new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether changes are made or not :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = sm_cls(environment.conf, **sm_configs['my_mode']) # 'default' or 'exact'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward or inverse model ?\n",
    "\n",
    "The forward model uses the dataset for the forward prediction computation, and the inverse model uses the forward model, or directly the dataset to perform inverse prediction. In other words, forward models predict $s_p$ given a $m$ that might have never been observed, using the dataset of observations $(m,s)$ and inverse models infer a motor command $m$ that should be able to reach a given goal $s_g$.  \n",
    "\n",
    "For each of these models it is necessary to load the mode with the appropriate model chosen as explained earlier and then create a dataset of n $(m,s)$ couples which is going to be tested later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creation of the dataset\n",
    "random_motors = environment.random_motors(n=1000)\n",
    "   \n",
    "for m in random_motors:\n",
    "    s = environment.compute_sensori_effect(m)\n",
    "    model.update(m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward models using guide\n",
    "\n",
    "The predicted sensorimotor effect $s_p$ of a command $m$ depending on a given model can be obtained this way :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_pred = model.forward_prediction(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test foward models, we are going to generate randomly n $m$ and use one of these models to find out $s_p$. The distance between $s_g$ and the time processing are weighted and saved in $comparison$ dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f153ada2fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Test of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_motors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtimer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'environment' is not defined"
     ]
    }
   ],
   "source": [
    "#Test of the model\n",
    "test = environment.random_motors(n=50)\n",
    "distance = []\n",
    "timer = []\n",
    "\n",
    "for mTest in test :\n",
    "    start = time.time()\n",
    "    sTest_pred = model.forward_prediction(mTest)\n",
    "    end = time.time()\n",
    "    sTest_goal = environment.compute_sensori_effect(mTest)\n",
    "    distance.append(spatial.distance.pdist([sTest_pred,sTest_goal])[0])\n",
    "    timer.append(end - start)\n",
    "    \n",
    "# Distance between the predictive and effectiv value\n",
    "comparison = {} #to remove after the first use\n",
    "comparison['my_model'] = [np.mean(distance), np.std(distance) ,max(distance),\n",
    "                    np.mean(timer), np.std(timer), max(timer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbor forward model\n",
    "\n",
    "To perform a forward prediction, the Nearest Neighbor model looks in the dataset of tuples $(m, s)$ for the nearest neighbor of the given $m$ motor command, and returns its corresponding $s$.\n",
    "The algorithm comes from scipy library : [scipy.spatial.KDTree.query](http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.spatial.KDTree.query.html) (with x = $s$, k=1, eps = 0, p = 2, radius = +inf). It returns distance of found nearest neighbors $s_p$.\n",
    "It works sufficiently well in different typical robotic applications but can be very long if the dataset's size exceed $10⁵$. \n",
    "\n",
    "Let's see how to use it : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Nearest Neighbor forward model\n",
    "\n",
    "To perform a forward prediction of $m$, the Weighted Nearest Neighbor model looks for the $k$ (parameter) nearest neighbors of $m$ in the dataset, and returns the average of the $k$ corresponding $s$. This average is weighted by the distance to $m$ with a gaussian of standard deviation $\\sigma$ (parameter). It finds the $k$ nearest neighbors (with the same algorithm than for NN model :[scipy.spatial.KDTree.query](http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.spatial.KDTree.query.html), with k = n the number of interesting neigbhors )  before weights them. It returns the n-dimensional weighted neighbor $s_p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Locally Weighted Linear Regression forward model\n",
    "\n",
    "The LWLR computes a linear regression of the $k$ nearest neighbors of $m$ and finds the requested $s_p$ given $m$. As before, it is possible to change the number $k$ of nearest neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward models for Non-Stationnary environments\n",
    "\n",
    "'NSNN' and 'NSLWLR' are modified versions of 'NN' and 'LWLR' where points are not only weighted by distance but also by the number of points that appeared after that one (gaussian with parameter sigma_t=100), to put less weight on old points and allow the learning of Non-Stationnary environments. *How to use it ?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward models comparisons\n",
    "#### Distance between $s_p$ and $s$ comparison\n",
    "\n",
    "If we look at comparisons values with a 1000 training $(m,s)$, we can easily notice that LWLR is closer to the goal on average than NN or WNN model. NN is really further than the two others which are not so different and the 3 standard deviation is equivalent. And the maximum distance can be sometimes very far away between the prediction and the goal for NN models. With a bigger dataset (10000), NN gets better results and is closest to his neighbors even if they are still better. LWLR has still the best average but WNN has a significant better standard deviation and a smallest maximum distance. To conclude, LWLR looks better in average but seems to be less good sometimes than the WNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time processing comparison\n",
    "\n",
    "NN model is far away better than his neighbor in speed processing in any case : around 4 times better than LWLR model and twice better than WNN with an equal standard deviation. These results were forseeable knowing that LWLR model uses WNN once which himself uses NN search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Inverse models using guide\n",
    "\n",
    "The predicted motor command $m_p$ to reach a goal $s_g$ can be obtained this way :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_pred = model.inverse_prediction(s_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test foward models, we are going to generate randomly n $s_g$ and use one of these models to find out $m$. The distance between $s_g$ and the effective $s$ and the time processing are weighted and saved in $comparison$ dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f153ada2fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Test of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_motors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtimer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'environment' is not defined"
     ]
    }
   ],
   "source": [
    "#Test of the model\n",
    "test = environment.random_sensors(n=50)\n",
    "distance = []\n",
    "timer = []\n",
    "\n",
    "for sTest in test :\n",
    "    start = time.time()\n",
    "    mTest = model.inverse_prediction(sTest)\n",
    "    end = time.time()\n",
    "    sTest_goal = environment.compute_sensori_effect(mTest)\n",
    "    distance.append(spatial.distance.pdist([sTest,sTest_goal])[0])\n",
    "    timer.append(end - start)\n",
    "    \n",
    "# Distance between the predictive and effectiv value\n",
    "comparison = {} #to remove after the first use\n",
    "comparison['my_model'] = [np.mean(distance), np.std(distance) ,max(distance),\n",
    "                    np.mean(timer), np.std(timer), max(timer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Nearest Neighbor inverse model\n",
    "\n",
    "\n",
    "To perform the inverse inference, the Nearest Neighbor inverse model looks in the dataset of tuples $(m, s)$, the nearest neighbor of the given $s$ motor command, and return its corresponding $m$.\n",
    "It works sufficiently well in different typical robotic applications but can be very long if the dataset's size exceed $10⁵$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  WNN Inverse Model\n",
    "\n",
    "Typical robotic forward models are very redundant: e.g. a robotic arm can put its hand to position $s$ with an infinity of possible $m$ motor positions.\n",
    "Thus, trying to infer a motor command $m$ to reach a given goal $s$ doing an average of the nearest neighbors of $s$ in the dataset would make no sense as those nearest neighbors might have very different corresponding motor commands.\n",
    "But to perform the inverse inference of a given $s$, the Weighted Nearest Neighbor model looks at the nearest neighbor of $s$ in the dataset and gets its corresponding $m$. It finds now the $k$ nearest neighbors of $m$ in the dataset, and returns their average weighted by the distance of their sensory part to $s$, with a gaussian of standard deviation $\\sigma$ (parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Optimization Inverse Model (BFGS and CMAES)\n",
    "Another possibility to perform inverse inference is to use an optimization algorithm to minimize the error  $e(m) = ||f(m) - s_g||^2$  where $s_g$ is the goal, $f$ is the forward model, and $m$ is the motor command to be infered.\n",
    "\n",
    "This is how our [scipy.optimize based](https://github.com/flowersteam/explauto/blob/master/explauto/sensorimotor_model/inverse/sciopt.py#L8) inverse models do. \n",
    "\n",
    "They take a 'maxfun' ([BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)) or 'maxiter' ([COBYLA] (https://en.wikipedia.org/wiki/COBYLA)) parameter that limits the number of error function (and so forward model) evaluation.\n",
    "\n",
    "The Covariance Matrix Adaptation -Evolutuionary Strategy optimizes that error function by making fewer assumptions on the regularity of the forward model to perform the search. It is based on a random exploration (with a computed covariance) around a current point of interest, and adapts this point and recompute the covariance matrix at each iteration, with memory of the taken path.\n",
    "The initial point is set as the motor part $m$ of the nearest neighbor $s$ of the goal $s_g$, and the initial covariance matrix is identity times an exploration $\\sigma$ (parameter). This inverse model also takes a 'maxfevals' parameter that limits the number of forward model evaluations.\n",
    "The method should be applied, if BFGS fails due to a rugged search landscape (e.g. discontinuities, sharp bends or ridges, noise, local optima, outliers). If second order derivative based methods are successful, they are usually faster than the CMA-ES: on purely convex-quadratic functions, BFGS is typically faster by a factor of about ten (in terms of number of objective function evaluations needed to reach a target function value, assuming that gradients are not available). \n",
    "\n",
    "See [Hansen's website](https://www.lri.fr/~hansen/cmaesintro.html) and this [tutorial](https://www.lri.fr/~hansen/cmatutorial.pdf) on CMA-ES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse models comparisons\n",
    "\n",
    "#### Distance between $s_p$ and $s$ comparison\n",
    "\n",
    "#### Time processing comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sensorimotor models: ['LWLR-BFGS', 'nearest_neighbor', 'WNN', 'LWLR-CMAES']\n",
      "LWLR 0 0.0686541978662 WNN 0 0.0884176480287 NN 0 0.177208535905 \n",
      "\n",
      "LWLR 1 0.0541419200709 WNN 1 0.0402551322972 NN 1 0.122515755016 \n",
      "\n",
      "LWLR 2 0.296264678198 WNN 2 0.202203225994 NN 2 0.454557748391 \n",
      "\n",
      "LWLR 3 0.0111941003799 WNN 3 0.000906777381897 NN 3 0.000500750541687 \n",
      "\n",
      "LWLR 4 0.0698805950638 WNN 4 0.00157149520023 NN 4 0.00172045156063 \n",
      "\n",
      "LWLR 5 0.500254869461 WNN 5 0.011901140213 NN 5 0.0125389099121 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from explauto import Environment\n",
    "environment = Environment.from_configuration('simple_arm', 'low_dimensional')\n",
    "\n",
    "from explauto.sensorimotor_model import sensorimotor_models\n",
    "print 'Available sensorimotor models: {}'.format(sensorimotor_models.keys())\n",
    "\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Environment definition\n",
    "from explauto.environment.environment import Environment\n",
    "environment = Environment.from_configuration('simple_arm', 'mid_dimensional')\n",
    "\n",
    "from explauto.sensorimotor_model import sensorimotor_models\n",
    "from explauto import SensorimotorModel\n",
    "\n",
    "#Creation of the dataset\n",
    "random_motors = environment.random_motors(n=10000)\n",
    "    \n",
    "sm_cls, sm_configs = sensorimotor_models['nearest_neighbor']\n",
    "model = sm_cls(environment.conf, **sm_configs['default'])\n",
    "\n",
    "for m in random_motors:\n",
    "    s = environment.compute_sensori_effect(m)\n",
    "    model.update(m, s)\n",
    "    \n",
    "    \n",
    "#Test of the model\n",
    "test = environment.random_motors(n=50)\n",
    "distance = []\n",
    "timer = []\n",
    "\n",
    "for mTest in test :\n",
    "    start = time.time()\n",
    "    sTest_pred = model.forward_prediction(mTest)\n",
    "    end = time.time()\n",
    "    sTest_eff = environment.compute_sensori_effect(mTest)\n",
    "    distance.append(spatial.distance.pdist([sTest_pred,sTest_eff])[0])\n",
    "    timer.append(end - start)\n",
    "# Distance between the predictive and effectiv value\n",
    "comparison = {}\n",
    "comparison['NN'] = [np.mean(distance), np.std(distance) ,max(distance),\n",
    "                    np.mean(timer), np.std(timer), max(timer)]\n",
    "#print comparison['NN']\n",
    "\n",
    "#Creation of the dataset\n",
    "random_motors = environment.random_motors(n=10000)\n",
    "    \n",
    "sm_cls, sm_configs = sensorimotor_models['WNN']\n",
    "model = sm_cls(environment.conf, **sm_configs['default'])\n",
    "\n",
    "for m in random_motors:\n",
    "    s = environment.compute_sensori_effect(m)\n",
    "    model.update(m, s)\n",
    "#Test of the model\n",
    "test = environment.random_motors(n=50)\n",
    "distance = []\n",
    "timer = []\n",
    "\n",
    "for mTest in test :\n",
    "    start = time.time()\n",
    "    sTest_pred = model.forward_prediction(mTest)\n",
    "    end = time.time()\n",
    "    sTest_eff = environment.compute_sensori_effect(mTest)\n",
    "    distance.append(spatial.distance.pdist([sTest_pred,sTest_eff])[0])\n",
    "    timer.append(end - start)\n",
    "\n",
    "# Distance between the predictive and effectiv value\n",
    "comparison['WNN'] = [np.mean(distance), np.std(distance) ,max(distance),\n",
    "                    np.mean(timer), np.std(timer), max(timer)]\n",
    "#print comparison['NN']\n",
    "#Creation of the dataset\n",
    "random_motors = environment.random_motors(n=10000)\n",
    "    \n",
    "sm_cls, sm_configs = sensorimotor_models['LWLR-BFGS']\n",
    "model = sm_cls(environment.conf, **sm_configs['default'])\n",
    "\n",
    "for m in random_motors:\n",
    "    s = environment.compute_sensori_effect(m)\n",
    "    model.update(m, s)\n",
    "#Test of the model\n",
    "test = environment.random_motors(n=50)\n",
    "distance = []\n",
    "timer = []\n",
    "\n",
    "for mTest in test :\n",
    "    start = time.time()\n",
    "    sTest_pred = model.forward_prediction(mTest)\n",
    "    end = time.time()\n",
    "    sTest_eff = environment.compute_sensori_effect(mTest)\n",
    "    distance.append(spatial.distance.pdist([sTest_pred,sTest_eff])[0])\n",
    "    timer.append(end - start)\n",
    "    \n",
    "# Distance between the predictive and effectiv value\n",
    "comparison['LWLR'] = [np.mean(distance), np.std(distance) ,max(distance),\n",
    "                    np.mean(timer), np.std(timer), max(timer)]\n",
    "#print comparison['NN']\n",
    "\n",
    "for i in range(0, 6) :\n",
    "    for c in comparison.keys():\n",
    "        print c, i, comparison[c][i],\n",
    "    print '\\n'\n",
    "        \n",
    "#print comparison['NN']\n",
    "#print comparison['WNN']\n",
    "#print comparison['LWLR']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
